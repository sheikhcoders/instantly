{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a977d2c7",
   "metadata": {},
   "source": [
    "# Agentic RAG with Instantly\n",
    "\n",
    "This notebook demonstrates how to build an advanced Retrieval-Augmented Generation (RAG) system using the Instantly library for inference. We'll create an intelligent agent that can retrieve and reason over documentation to answer questions accurately.\n",
    "\n",
    "Key features of this implementation:\n",
    "- Multi-step reasoning\n",
    "- Iterative retrieval\n",
    "- Query optimization\n",
    "- Self-critique and refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309c2027",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "First, let's install the required packages and configure our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42fc767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install instantly langchain langchain-community sentence-transformers datasets python-dotenv rank_bm25 --upgrade\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from instantly import OpenAIClient\n",
    "import datasets\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "# Load environment variables (HF_TOKEN)\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Instantly client\n",
    "client = OpenAIClient(\n",
    "    base_url=\"https://router.huggingface.co/v1\",\n",
    "    api_key=os.environ[\"HF_TOKEN\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9759a22d",
   "metadata": {},
   "source": [
    "## 2. Knowledge Base Preparation\n",
    "\n",
    "Now we'll prepare our knowledge base by loading and processing documents. We'll use the Hugging Face documentation dataset as our source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e185b839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Hugging Face documentation dataset\n",
    "knowledge_base = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n",
    "\n",
    "# Filter to include only Transformers documentation\n",
    "knowledge_base = knowledge_base.filter(lambda row: row[\"source\"].startswith(\"huggingface/transformers\"))\n",
    "\n",
    "# Convert dataset entries to Document objects with metadata\n",
    "source_docs = [\n",
    "    Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"].split(\"/\")[1]})\n",
    "    for doc in knowledge_base\n",
    "]\n",
    "\n",
    "# Create text splitter for processing documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # Characters per chunk\n",
    "    chunk_overlap=50,  # Overlap between chunks to maintain context\n",
    "    add_start_index=True,\n",
    "    strip_whitespace=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]  # Priority order for splitting\n",
    ")\n",
    "\n",
    "# Process documents into chunks\n",
    "docs_processed = text_splitter.split_documents(source_docs)\n",
    "\n",
    "print(f\"Knowledge base prepared with {len(docs_processed)} document chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ffceab",
   "metadata": {},
   "source": [
    "## 3. Build Retriever Tool\n",
    "\n",
    "We'll create a custom retriever tool that our agent can use to search the knowledge base. This tool will use BM25 for fast and effective retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cce255",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrieverTool:\n",
    "    \"\"\"A tool for retrieving relevant documents from the knowledge base.\"\"\"\n",
    "    \n",
    "    def __init__(self, docs, max_results=5):\n",
    "        self.retriever = BM25Retriever.from_documents(docs, k=max_results)\n",
    "        \n",
    "    def search(self, query: str) -> str:\n",
    "        \"\"\"Search the knowledge base with the given query.\"\"\"\n",
    "        # Retrieve relevant documents\n",
    "        docs = self.retriever.get_relevant_documents(query)\n",
    "        \n",
    "        # Format results\n",
    "        results = []\n",
    "        for i, doc in enumerate(docs, 1):\n",
    "            results.append(f\"\\n=== Document {i} ===\\n{doc.page_content}\\nSource: {doc.metadata['source']}\\n\")\n",
    "            \n",
    "        return \"\\n\".join(results)\n",
    "\n",
    "# Initialize retriever tool\n",
    "retriever = RetrieverTool(docs_processed)\n",
    "\n",
    "# Test the retriever\n",
    "test_query = \"How does model training work?\"\n",
    "print(retriever.search(test_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d1ba3b",
   "metadata": {},
   "source": [
    "## 4. Create Agent with RAG Capabilities\n",
    "\n",
    "Now we'll create an agent that can use our retriever tool to answer questions. We'll use Instantly's OpenAI-compatible interface to interact with Hugging Face models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c3300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGAgent:\n",
    "    \"\"\"An agent that combines retrieval with language model reasoning.\"\"\"\n",
    "    \n",
    "    def __init__(self, client: OpenAIClient, retriever: RetrieverTool):\n",
    "        self.client = client\n",
    "        self.retriever = retriever\n",
    "        \n",
    "    def _format_prompt(self, query: str, context: str) -> list:\n",
    "        \"\"\"Format the conversation prompt with retrieved context.\"\"\"\n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant. Use the provided context to answer questions accurately. If you're not sure about something, say so.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {query}\"}\n",
    "        ]\n",
    "    \n",
    "    def answer(self, query: str) -> str:\n",
    "        \"\"\"Answer a question using retrieved context and LLM reasoning.\"\"\"\n",
    "        # First, get relevant context\n",
    "        context = self.retriever.search(query)\n",
    "        \n",
    "        # Generate response using the LLM\n",
    "        messages = self._format_prompt(query, context)\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"HuggingFaceH4/zephyr-7b-beta\",  # You can change this to other models\n",
    "            messages=messages,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "\n",
    "# Initialize the RAG agent\n",
    "agent = RAGAgent(client, retriever)\n",
    "\n",
    "# Test the agent\n",
    "test_question = \"What are the key steps in fine-tuning a transformer model?\"\n",
    "print(f\"Question: {test_question}\\n\")\n",
    "print(f\"Answer: {agent.answer(test_question)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c11ab26",
   "metadata": {},
   "source": [
    "## 5. Advanced RAG Features: HyDE and Query Refinement\n",
    "\n",
    "Let's enhance our RAG system with two advanced techniques:\n",
    "1. Hypothetical Document Embedding (HyDE): Generate a hypothetical answer first to improve retrieval\n",
    "2. Self-Query Refinement: Analyze and refine queries based on initial results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aa4609",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedRAGAgent(RAGAgent):\n",
    "    \"\"\"Enhanced RAG agent with HyDE and query refinement capabilities.\"\"\"\n",
    "    \n",
    "    def _generate_hypothetical_document(self, query: str) -> str:\n",
    "        \"\"\"Generate a hypothetical document that might contain the answer (HyDE).\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Given a question, write a short technical document that would contain its answer.\"},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "            messages=messages,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    def _refine_query(self, query: str, initial_results: str) -> str:\n",
    "        \"\"\"Analyze initial results and generate a refined query.\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Analyze the search results and suggest a refined search query to find more relevant information.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Original query: {query}\\n\\nInitial results:\\n{initial_results}\"}\n",
    "        ]\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "            messages=messages,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    def answer(self, query: str) -> str:\n",
    "        \"\"\"Enhanced answer method using HyDE and query refinement.\"\"\"\n",
    "        # Step 1: Generate hypothetical document\n",
    "        hyde_doc = self._generate_hypothetical_document(query)\n",
    "        \n",
    "        # Step 2: Use hypothetical document to get initial results\n",
    "        initial_results = self.retriever.search(hyde_doc)\n",
    "        \n",
    "        # Step 3: Refine the query based on initial results\n",
    "        refined_query = self._refine_query(query, initial_results)\n",
    "        \n",
    "        # Step 4: Get additional results with refined query\n",
    "        additional_results = self.retriever.search(refined_query)\n",
    "        \n",
    "        # Step 5: Combine all context\n",
    "        combined_context = f\"Initial results:\\n{initial_results}\\n\\nAdditional results:\\n{additional_results}\"\n",
    "        \n",
    "        # Step 6: Generate final answer\n",
    "        messages = self._format_prompt(query, combined_context)\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "            messages=messages,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "\n",
    "# Initialize the advanced RAG agent\n",
    "advanced_agent = AdvancedRAGAgent(client, retriever)\n",
    "\n",
    "# Test the advanced agent with a complex question\n",
    "complex_question = \"What are the trade-offs between different attention mechanisms in transformers?\"\n",
    "print(f\"Question: {complex_question}\\n\")\n",
    "print(f\"Answer: {advanced_agent.answer(complex_question)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f407af",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've built an advanced RAG system that:\n",
    "1. Uses Instantly to interface with powerful language models\n",
    "2. Implements sophisticated retrieval with BM25\n",
    "3. Enhances results using HyDE and query refinement\n",
    "4. Provides transparent, source-backed responses\n",
    "\n",
    "Try experimenting with different:\n",
    "- Language models (via Instantly's HuggingFace integration)\n",
    "- Retrieval methods (e.g., embedding-based search)\n",
    "- Prompt strategies\n",
    "- Document processing approaches\n",
    "\n",
    "The combination of retrieval and reasoning enables more accurate, trustworthy, and capable AI systems."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
