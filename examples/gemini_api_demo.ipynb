{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a9672ca",
   "metadata": {},
   "source": [
    "# Google Gemini API Integration with Instantly\n",
    "\n",
    "This notebook demonstrates how to use Google's Gemini API through the Instantly library. We'll explore various features including text generation, chat conversations, multimodal inputs, and model information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17f5103",
   "metadata": {},
   "source": [
    "## Authentication Setup\n",
    "\n",
    "First, let's set up our environment and initialize the Google AI client using the Instantly library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9f92f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from instantly import GoogleAIClient\n",
    "\n",
    "# Initialize the client with your API key\n",
    "client = GoogleAIClient(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# You can also set the API key in your environment:\n",
    "# os.environ[\"GEMINI_API_KEY\"] = \"your-api-key-here\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6fe7b7",
   "metadata": {},
   "source": [
    "## Standard Content Generation\n",
    "\n",
    "Let's start with basic text generation using the `generateContent` endpoint. This is best for non-interactive tasks where you can wait for the complete response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1a0248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple text generation with default parameters\n",
    "response = client.chat_completion(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain how AI works in a single paragraph.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Response:\", response[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "# Text generation with customized parameters\n",
    "response = client.chat_completion(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Write a creative story about a robot learning to paint.\"}\n",
    "    ],\n",
    "    temperature=0.9,  # Higher temperature for more creative output\n",
    "    thinking_config={\"thinking_budget\": -1}  # No limit on thinking time\n",
    ")\n",
    "\n",
    "print(\"\\nCreative story:\", response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5681b5",
   "metadata": {},
   "source": [
    "## Streaming Content Generation\n",
    "\n",
    "For interactive applications like chatbots, the streaming API provides a better user experience by returning content chunks as they're generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4d3f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream content generation\n",
    "stream = client.stream_chat_completion(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Write a poem about coding, one line at a time.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Streaming response:\")\n",
    "for chunk in stream:\n",
    "    if \"content\" in chunk[\"choices\"][0][\"delta\"]:\n",
    "        print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1961b05a",
   "metadata": {},
   "source": [
    "## Chat Conversations\n",
    "\n",
    "For multi-turn conversations, we need to maintain the chat history by appending each message to the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d0cc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize conversation history\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant who likes to explain complex topics simply.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi! Can you help me understand quantum computing?\"}\n",
    "]\n",
    "\n",
    "# Get first response\n",
    "response = client.chat_completion(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    messages=conversation\n",
    ")\n",
    "assistant_message = response[\"choices\"][0][\"message\"]\n",
    "conversation.append(assistant_message)\n",
    "print(\"Assistant:\", assistant_message[\"content\"])\n",
    "\n",
    "# Continue conversation\n",
    "conversation.append({\"role\": \"user\", \"content\": \"Can you give a simple example of what a qubit is?\"})\n",
    "response = client.chat_completion(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    messages=conversation\n",
    ")\n",
    "assistant_message = response[\"choices\"][0][\"message\"]\n",
    "conversation.append(assistant_message)\n",
    "print(\"\\nAssistant:\", assistant_message[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb93904",
   "metadata": {},
   "source": [
    "## Multimodal Inputs\n",
    "\n",
    "Gemini can process both text and images in a single prompt. Here's how to send an image along with text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e9116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Example of analyzing an image (you would need a real image file)\n",
    "\"\"\"\n",
    "image_path = \"path/to/your/image.jpg\"\n",
    "image_data = encode_image(image_path)\n",
    "\n",
    "response = client.chat_completion(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"inline_data\",\n",
    "                    \"mime_type\": \"image/jpeg\",\n",
    "                    \"data\": image_data\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What can you see in this image?\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Analysis:\", response[\"choices\"][0][\"message\"][\"content\"])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dedbd2e",
   "metadata": {},
   "source": [
    "## Model Information\n",
    "\n",
    "Let's explore how to get information about available models and their capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b685fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available models\n",
    "models = client.list_models()\n",
    "print(\"Available models:\")\n",
    "for model in models[\"models\"]:\n",
    "    print(f\"\\nModel: {model['name']}\")\n",
    "    if \"description\" in model:\n",
    "        print(f\"Description: {model['description']}\")\n",
    "    print(f\"Token limits: Input={model.get('inputTokenLimit', 'N/A')}, Output={model.get('outputTokenLimit', 'N/A')}\")\n",
    "    print(f\"Supported methods: {', '.join(model.get('supportedGenerationMethods', []))}\")\n",
    "    print(f\"Temperature range: 0.0 to {model.get('maxTemperature', 1.0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fa5179",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Here are some ideas for further exploration:\n",
    "- Experiment with different model parameters like temperature and top_p\n",
    "- Try more complex multimodal prompts combining text and images\n",
    "- Implement error handling and retries for production use\n",
    "- Explore specialized models for tasks like image generation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
